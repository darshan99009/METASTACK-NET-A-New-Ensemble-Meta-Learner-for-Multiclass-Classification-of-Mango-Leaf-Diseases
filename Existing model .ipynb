{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMS/t7V5DZQ3vEpjsejo+kb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"99d97a223b634f2f96a37a2eaff6bb9c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9f3d8ac19a244be94ab1b5932905300","IPY_MODEL_f3a3d5b8b1c74bfda8c555cf3b053f15","IPY_MODEL_23cb4f4d2c3548df972ce1e36a023edb"],"layout":"IPY_MODEL_e429ebbca88b41d69d2fd998a8081c2a"}},"c9f3d8ac19a244be94ab1b5932905300":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0ab2ddc234548a4a1152c90b707f952","placeholder":"​","style":"IPY_MODEL_ac67d310cc1946cab62d614f0da85d0b","value":"config.json: "}},"f3a3d5b8b1c74bfda8c555cf3b053f15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7dcd0812cf544d5b08096e67887b103","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42166bc106244abd832c1075617a6cdf","value":1}},"23cb4f4d2c3548df972ce1e36a023edb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_456484a543fa404a8f35cb89c0834f85","placeholder":"​","style":"IPY_MODEL_53b42780b1f241a78eaa894159f27b87","value":" 69.7k/? [00:00&lt;00:00, 6.41MB/s]"}},"e429ebbca88b41d69d2fd998a8081c2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0ab2ddc234548a4a1152c90b707f952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac67d310cc1946cab62d614f0da85d0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7dcd0812cf544d5b08096e67887b103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"42166bc106244abd832c1075617a6cdf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"456484a543fa404a8f35cb89c0834f85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53b42780b1f241a78eaa894159f27b87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbc5f209b8274ed2b207002cf6935c7b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_162069a88b0e4ea3954cdf9469348489","IPY_MODEL_b0f9a93d743d4557a4b6492131cf0c0c","IPY_MODEL_d010d8b518874298befd68ac4613bcc1"],"layout":"IPY_MODEL_4b120c2e28c7479b9cc676d7e9067800"}},"162069a88b0e4ea3954cdf9469348489":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3fc243a423544f2bfd2481b853f0ff7","placeholder":"​","style":"IPY_MODEL_dcacdcfc2e8246b6a8d9094601b71663","value":"config.json: "}},"b0f9a93d743d4557a4b6492131cf0c0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18ae57a1ea7e4fcabe5776822a235834","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f003ea99e334ca38207af6aaf825ad9","value":1}},"d010d8b518874298befd68ac4613bcc1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_758c21731f884d10bc333606c2193340","placeholder":"​","style":"IPY_MODEL_4ccecfbbc9da4064b599d4b4efe316f6","value":" 71.8k/? [00:00&lt;00:00, 5.49MB/s]"}},"4b120c2e28c7479b9cc676d7e9067800":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3fc243a423544f2bfd2481b853f0ff7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcacdcfc2e8246b6a8d9094601b71663":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18ae57a1ea7e4fcabe5776822a235834":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"2f003ea99e334ca38207af6aaf825ad9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"758c21731f884d10bc333606c2193340":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ccecfbbc9da4064b599d4b4efe316f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["99d97a223b634f2f96a37a2eaff6bb9c","c9f3d8ac19a244be94ab1b5932905300","f3a3d5b8b1c74bfda8c555cf3b053f15","23cb4f4d2c3548df972ce1e36a023edb","e429ebbca88b41d69d2fd998a8081c2a","f0ab2ddc234548a4a1152c90b707f952","ac67d310cc1946cab62d614f0da85d0b","e7dcd0812cf544d5b08096e67887b103","42166bc106244abd832c1075617a6cdf","456484a543fa404a8f35cb89c0834f85","53b42780b1f241a78eaa894159f27b87","bbc5f209b8274ed2b207002cf6935c7b","162069a88b0e4ea3954cdf9469348489","b0f9a93d743d4557a4b6492131cf0c0c","d010d8b518874298befd68ac4613bcc1","4b120c2e28c7479b9cc676d7e9067800","b3fc243a423544f2bfd2481b853f0ff7","dcacdcfc2e8246b6a8d9094601b71663","18ae57a1ea7e4fcabe5776822a235834","2f003ea99e334ca38207af6aaf825ad9","758c21731f884d10bc333606c2193340","4ccecfbbc9da4064b599d4b4efe316f6"],"output_embedded_package_id":"11qXmXPYCiW6lX2XYUH6uYcV2cAPpoPt4"},"id":"_MNTVZdXMiM4","executionInfo":{"status":"ok","timestamp":1757530916127,"user_tz":-330,"elapsed":4733107,"user":{"displayName":"Face swap","userId":"01531077573911084269"}},"outputId":"cbcc28ab-fc1b-4512-8569-d126760fd392"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Install required packages\n","!pip install torch torchvision transformers timm pandas matplotlib seaborn tqdm fpdf grad-cam scikit-plot\n","!apt-get install -y python3-dev python3-setuptools python3-wheel\n","\n","import os\n","import zipfile\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms, models\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n","from sklearn.preprocessing import label_binarize\n","from itertools import cycle\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Import for Grad-CAM\n","from pytorch_grad_cam import GradCAM\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Extract the dataset\n","!unzip -o -q mango_leaf_dataset.zip -d mango_leaf_dataset\n","\n","# Define the dataset class\n","class MangoLeafDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.classes = ['Anthracnose', 'Bacterial Canker', 'Cutting Weevil',\n","                       'Die Back', 'Gall Midge', 'Healthy', 'Powdery Mildew', 'Sooty Mould']\n","        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n","        self.images = []\n","        self.labels = []\n","\n","        # Load images\n","        for cls in self.classes:\n","            cls_dir = os.path.join(root_dir, cls)\n","            if os.path.exists(cls_dir):\n","                for img_name in os.listdir(cls_dir):\n","                    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n","                        self.images.append(os.path.join(cls_dir, img_name))\n","                        self.labels.append(self.class_to_idx[cls])\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.images[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label, img_path  # Added img_path for Grad-CAM\n","\n","# Hyperparameters from the paper\n","BATCH_SIZE = 32\n","LEARNING_RATE = 0.001\n","NUM_EPOCHS = 10\n","IMAGE_SIZE = 224\n","MEAN = [0.485, 0.456, 0.406]\n","STD = [0.229, 0.224, 0.225]\n","DROPOUT = 0.5\n","SCHEDULER_STEP_SIZE = 7\n","SCHEDULER_GAMMA = 0.1\n","\n","# Data transformations\n","train_transform = transforms.Compose([\n","    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=MEAN, std=STD)\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=MEAN, std=STD)\n","])\n","\n","# Load dataset\n","data_dir = \"mango_leaf_dataset/DataSet\"\n","dataset = MangoLeafDataset(data_dir, transform=train_transform)\n","\n","# Split dataset (80% train, 10% validation, 10% test)\n","train_size = int(0.8 * len(dataset))\n","val_size = int(0.1 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","\n","train_dataset, val_dataset, test_dataset = random_split(\n","    dataset, [train_size, val_size, test_size],\n","    generator=torch.Generator().manual_seed(42)\n",")\n","\n","# Apply test transform to validation and test sets\n","val_dataset.dataset.transform = test_transform\n","test_dataset.dataset.transform = test_transform\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","print(f\"Total images: {len(dataset)}\")\n","print(f\"Train size: {len(train_dataset)}\")\n","print(f\"Validation size: {len(val_dataset)}\")\n","print(f\"Test size: {len(test_dataset)}\")\n","\n","# Function to visualize samples\n","def visualize_samples(dataset, num_samples=5):\n","    fig, axes = plt.subplots(len(dataset.classes), num_samples, figsize=(15, 12))\n","\n","    for cls_idx, cls_name in enumerate(dataset.classes):\n","        # Get samples for this class\n","        class_indices = [i for i, label in enumerate(dataset.labels) if label == cls_idx]\n","        sample_indices = np.random.choice(class_indices, num_samples, replace=False)\n","\n","        for i, idx in enumerate(sample_indices):\n","            img_path = dataset.images[idx]\n","            image = Image.open(img_path).convert('RGB')\n","\n","            axes[cls_idx, i].imshow(image)\n","            axes[cls_idx, i].axis('off')\n","            if i == 0:\n","                axes[cls_idx, i].set_ylabel(cls_name, fontsize=12)\n","\n","    plt.tight_layout()\n","    plt.savefig('sample_images.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","    plt.close()\n","\n","# Function to plot class distribution\n","def plot_class_distribution(dataset):\n","    class_counts = [dataset.labels.count(i) for i in range(len(dataset.classes))]\n","\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(dataset.classes, class_counts)\n","    plt.title('Class Distribution in Dataset')\n","    plt.xlabel('Class')\n","    plt.ylabel('Number of Images')\n","    plt.xticks(rotation=45)\n","    plt.savefig('class_distribution.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","    plt.close()\n","\n","    return class_counts\n","\n","# Modified model training and evaluation functions to handle Hugging Face models\n","def train_model(model, model_name, train_loader, val_loader, num_epochs=NUM_EPOCHS):\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n","\n","    train_losses = []\n","    val_losses = []\n","    train_accs = []\n","    val_accs = []\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels, _ in tqdm(train_loader, desc=f\"{model_name} Epoch {epoch+1}/{num_epochs}\"):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Handle different model output formats\n","            if model_name in ['ViT', 'Swin Transformer']:\n","                outputs = model(images)\n","                logits = outputs.logits\n","            else:\n","                logits = model(images)\n","\n","            loss = criterion(logits, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(logits.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        scheduler.step()\n","        train_loss = running_loss / len(train_loader)\n","        train_acc = 100 * correct / total\n","        train_losses.append(train_loss)\n","        train_accs.append(train_acc)\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for images, labels, _ in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","\n","                # Handle different model output formats\n","                if model_name in ['ViT', 'Swin Transformer']:\n","                    outputs = model(images)\n","                    logits = outputs.logits\n","                else:\n","                    logits = model(images)\n","\n","                loss = criterion(logits, labels)\n","                val_loss += loss.item()\n","\n","                _, predicted = torch.max(logits.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(val_loader)\n","        val_acc = 100 * correct / total\n","        val_losses.append(val_loss)\n","        val_accs.append(val_acc)\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n","\n","        # Save best model\n","        if val_acc > best_acc:\n","            best_acc = val_acc\n","            torch.save(model.state_dict(), f'{model_name}_best.pth')\n","\n","    # Plot training history\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Val Loss')\n","    plt.title('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accs, label='Train Acc')\n","    plt.plot(val_accs, label='Val Acc')\n","    plt.title('Accuracy')\n","    plt.legend()\n","    plt.suptitle(f'{model_name} Training History')\n","    plt.savefig(f'{model_name}_training_history.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","    plt.close()\n","\n","    return best_acc, {\n","        'train_losses': train_losses,\n","        'val_losses': val_losses,\n","        'train_accs': train_accs,\n","        'val_accs': val_accs\n","    }\n","\n","def evaluate_model(model, model_name, test_loader):\n","    model.load_state_dict(torch.load(f'{model_name}_best.pth'))\n","    model.eval()\n","\n","    all_preds = []\n","    all_labels = []\n","    all_probs = []\n","    all_image_paths = []\n","\n","    with torch.no_grad():\n","        for images, labels, img_paths in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            # Handle different model output formats\n","            if model_name in ['ViT', 'Swin Transformer']:\n","                outputs = model(images)\n","                logits = outputs.logits\n","            else:\n","                logits = model(images)\n","\n","            probs = torch.nn.functional.softmax(logits, dim=1)\n","            _, predicted = torch.max(logits.data, 1)\n","\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_probs.extend(probs.cpu().numpy())\n","            all_image_paths.extend(img_paths)\n","\n","    # Calculate accuracy\n","    accuracy = 100 * np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=dataset.classes, yticklabels=dataset.classes)\n","    plt.title(f'{model_name} Confusion Matrix')\n","    plt.ylabel('True Label')\n","    plt.xlabel('Predicted Label')\n","    plt.savefig(f'{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","    plt.close()\n","\n","    # Classification report\n","    report = classification_report(all_labels, all_preds, target_names=dataset.classes, output_dict=True)\n","\n","    # ROC Curve\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","\n","    # Binarize the output for ROC curve\n","    y_test_bin = label_binarize(all_labels, classes=range(len(dataset.classes)))\n","    n_classes = y_test_bin.shape[1]\n","\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], np.array(all_probs)[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","    # Compute micro-average ROC curve and ROC area\n","    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), np.array(all_probs).ravel())\n","    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","    # Plot ROC curve\n","    plt.figure(figsize=(10, 8))\n","    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink'])\n","    for i, color in zip(range(n_classes), colors):\n","        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n","                 label='ROC curve of class {0} (area = {1:0.2f})'\n","                 ''.format(dataset.classes[i], roc_auc[i]))\n","\n","    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(f'{model_name} ROC Curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.savefig(f'{model_name}_roc_curve.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","    plt.close()\n","\n","    # Precision-Recall curve\n","    precision = dict()\n","    recall = dict()\n","    average_precision = dict()\n","\n","    for i in range(n_classes):\n","        precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], np.array(all_probs)[:, i])\n","        average_precision[i] = auc(recall[i], precision[i])\n","\n","    # Plot Precision-Recall curve\n","    plt.figure(figsize=(10, 8))\n","    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink'])\n","    for i, color in zip(range(n_classes), colors):\n","        plt.plot(recall[i], precision[i], color=color, lw=2,\n","                 label='Precision-Recall curve of class {0} (area = {1:0.2f})'\n","                 ''.format(dataset.classes[i], average_precision[i]))\n","\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.title(f'{model_name} Precision-Recall Curve')\n","    plt.legend(loc=\"lower left\")\n","    plt.savefig(f'{model_name}_pr_curve.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","    plt.close()\n","\n","    return accuracy, report, all_probs, all_image_paths, all_preds, all_labels, roc_auc, average_precision\n","\n","# Model definitions\n","def create_alexnet():\n","    model = models.alexnet(pretrained=True)\n","    model.classifier[6] = nn.Linear(4096, 8)  # 8 classes\n","    return model\n","\n","def create_resnet():\n","    model = models.resnet50(pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, 8)  # 8 classes\n","    return model\n","\n","def create_vgg16():\n","    model = models.vgg16(pretrained=True)\n","    model.classifier[6] = nn.Linear(4096, 8)  # 8 classes\n","    return model\n","\n","def create_vit():\n","    from transformers import ViTForImageClassification, ViTConfig\n","\n","    config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n","    config.num_labels = 8\n","    model = ViTForImageClassification(config)\n","    return model\n","\n","def create_swin():\n","    from transformers import SwinForImageClassification, SwinConfig\n","\n","    config = SwinConfig.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n","    config.num_labels = 8\n","    model = SwinForImageClassification(config)\n","    return model\n","\n","# Fusion models\n","class FusionModel(nn.Module):\n","    def __init__(self, model1, model2, num_classes=8):\n","        super(FusionModel, self).__init__()\n","        self.model1 = model1\n","        self.model2 = model2\n","\n","        # Remove the final classification layer from both models\n","        if hasattr(model1, 'classifier'):\n","            model1.classifier = nn.Sequential(*list(model1.classifier.children())[:-1])\n","        elif hasattr(model1, 'fc'):\n","            model1.fc = nn.Identity()\n","\n","        if hasattr(model2, 'classifier'):\n","            model2.classifier = nn.Sequential(*list(model2.classifier.children())[:-1])\n","        elif hasattr(model2, 'fc'):\n","            model2.fc = nn.Identity()\n","\n","        # Determine the output dimensions\n","        with torch.no_grad():\n","            dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n","            out1 = model1(dummy_input)\n","            out2 = model2(dummy_input)\n","            if isinstance(out1, tuple):  # For Hugging Face models\n","                out1 = out1.logits\n","            if isinstance(out2, tuple):  # For Hugging Face models\n","                out2 = out2.logits\n","\n","            self.feature_dim = out1.shape[1] + out2.shape[1]\n","\n","        # Final classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(self.feature_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(DROPOUT),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        features1 = self.model1(x)\n","        features2 = self.model2(x)\n","\n","        if isinstance(features1, tuple):  # For Hugging Face models\n","            features1 = features1.logits\n","        if isinstance(features2, tuple):  # For Hugging Face models\n","            features2 = features2.logits\n","\n","        combined = torch.cat((features1, features2), dim=1)\n","        return self.classifier(combined)\n","\n","def create_fusion_resnet_vgg16():\n","    model1 = create_resnet()\n","    model2 = create_vgg16()\n","    return FusionModel(model1, model2)\n","\n","def create_fusion_resnet_alexnet():\n","    model1 = create_resnet()\n","    model2 = create_alexnet()\n","    return FusionModel(model1, model2)\n","\n","def create_fusion_resnet_alexnet_gradcam():\n","    model1 = create_resnet()\n","    model2 = create_alexnet()\n","    return FusionModel(model1, model2)\n","\n","# Function to apply Grad-CAM\n","def apply_grad_cam(model, model_name, test_loader, results):\n","    model.load_state_dict(torch.load(f'{model_name}_best.pth'))\n","    model.eval()\n","\n","    # Create Grad-CAM object\n","    target_layers = []\n","    if hasattr(model, 'features'):\n","        target_layers = [model.features[-1]]\n","    elif hasattr(model, 'layer4'):\n","        target_layers = [model.layer4[-1]]\n","    elif hasattr(model, 'encoder'):\n","        target_layers = [model.encoder.layers[-1].norm1]\n","\n","    if not target_layers:\n","        print(f\"Could not find target layers for Grad-CAM in {model_name}\")\n","        return\n","\n","    cam = GradCAM(model=model, target_layers=target_layers)\n","\n","    # Get a batch of images\n","    images, labels, img_paths = next(iter(test_loader))\n","    images = images.to(device)\n","\n","    # Generate CAM for each image\n","    grayscale_cams = cam(input_tensor=images)\n","\n","    # Visualize and save CAM results\n","    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n","    for i, (image, grayscale_cam) in enumerate(zip(images, grayscale_cams)):\n","        if i >= 8:  # Only show 8 examples\n","            break\n","\n","        rgb_img = image.cpu().permute(1, 2, 0).numpy()\n","        rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n","\n","        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n","\n","        row, col = i // 4, i % 4\n","        axes[row, col].imshow(visualization)\n","        axes[row, col].axis('off')\n","        axes[row, col].set_title(f'True: {dataset.classes[labels[i]]}')\n","\n","    plt.tight_layout()\n","    plt.savefig(f'{model_name}_grad_cam.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","    plt.close()\n","\n","    print(f\"Grad-CAM visualization saved as {model_name}_grad_cam.png\")\n","\n","# Function to create a comprehensive PDF report at the end\n","def create_comprehensive_pdf(results, paper_results, dataset, train_dataset, val_dataset, test_dataset):\n","    from fpdf import FPDF\n","    import os\n","\n","    pdf = FPDF()\n","    pdf.set_auto_page_break(auto=True, margin=15)\n","\n","    # Title Page\n","    pdf.add_page()\n","    pdf.set_font(\"Arial\", 'B', 18)\n","    pdf.cell(0, 10, \"Mango Leaf Disease Classification\", 0, 1, \"C\")\n","    pdf.ln(10)\n","\n","    pdf.set_font(\"Arial\", '', 12)\n","    pdf.multi_cell(0, 10, \"This report consolidates the training, evaluation, and comparison \"\n","                          \"of different models (AlexNet, ResNet, VGG-16, ViT, Swin Transformer) \"\n","                          \"for Mango Leaf Disease Classification.\")\n","    pdf.ln(10)\n","\n","    # Dataset Information\n","    pdf.set_font(\"Arial\", 'B', 14)\n","    pdf.cell(0, 10, \"Dataset Information\", 0, 1)\n","    pdf.set_font(\"Arial\", '', 12)\n","    pdf.cell(0, 10, f\"Total images: {len(dataset)}\", 0, 1)\n","    pdf.cell(0, 10, f\"Training set: {len(train_dataset)}\", 0, 1)\n","    pdf.cell(0, 10, f\"Validation set: {len(val_dataset)}\", 0, 1)\n","    pdf.cell(0, 10, f\"Test set: {len(test_dataset)}\", 0, 1)\n","    pdf.ln(10)\n","\n","    # Add sample images\n","    if os.path.exists('sample_images.png'):\n","        pdf.add_page()\n","        pdf.set_font(\"Arial\", 'B', 12)\n","        pdf.cell(0, 10, \"Sample Images from Dataset\", 0, 1)\n","        pdf.image('sample_images.png', x=10, y=20, w=180)\n","        pdf.ln(100)\n","\n","    # Add class distribution\n","    if os.path.exists('class_distribution.png'):\n","        pdf.add_page()\n","        pdf.set_font(\"Arial\", 'B', 12)\n","        pdf.cell(0, 10, \"Class Distribution\", 0, 1)\n","        pdf.image('class_distribution.png', x=10, y=20, w=180)\n","        pdf.ln(100)\n","\n","    # Model Comparison Table\n","    pdf.add_page()\n","    pdf.set_font(\"Arial\", 'B', 14)\n","    pdf.cell(0, 10, \"Model Performance Comparison\", 0, 1, \"C\")\n","    pdf.set_font(\"Arial\", 'B', 12)\n","    pdf.cell(60, 10, \"Model\", 1)\n","    pdf.cell(40, 10, \"Our Accuracy (%)\", 1)\n","    pdf.cell(40, 10, \"Paper Accuracy (%)\", 1)\n","    pdf.cell(40, 10, \"Difference (%)\", 1)\n","    pdf.ln()\n","\n","    pdf.set_font(\"Arial\", '', 12)\n","    for model_name in results:\n","        our_acc = results[model_name][\"Test Accuracy\"]\n","        paper_acc = paper_results[model_name]\n","        diff = our_acc - paper_acc\n","        pdf.cell(60, 10, model_name, 1)\n","        pdf.cell(40, 10, f\"{our_acc:.2f}\", 1)\n","        pdf.cell(40, 10, f\"{paper_acc:.2f}\", 1)\n","        pdf.cell(40, 10, f\"{diff:+.2f}\", 1)\n","        pdf.ln()\n","    pdf.ln(10)\n","\n","    # Detailed comparison table with more metrics\n","    pdf.add_page()\n","    pdf.set_font(\"Arial\", 'B', 14)\n","    pdf.cell(0, 10, \"Detailed Model Comparison\", 0, 1, \"C\")\n","    pdf.set_font(\"Arial\", 'B', 10)\n","    pdf.cell(40, 10, \"Model\", 1)\n","    pdf.cell(25, 10, \"Accuracy\", 1)\n","    pdf.cell(25, 10, \"Precision\", 1)\n","    pdf.cell(25, 10, \"Recall\", 1)\n","    pdf.cell(25, 10, \"F1-Score\", 1)\n","    pdf.cell(25, 10, \"ROC AUC\", 1)\n","    pdf.cell(25, 10, \"Avg Precision\", 1)\n","    pdf.ln()\n","\n","    pdf.set_font(\"Arial\", '', 10)\n","    for model_name in results:\n","        report = results[model_name][\"Classification Report\"]\n","        roc_auc = results[model_name][\"ROC AUC\"][\"micro\"]\n","        avg_precision = np.mean(list(results[model_name][\"Average Precision\"].values()))\n","\n","        pdf.cell(40, 10, model_name, 1)\n","        pdf.cell(25, 10, f\"{results[model_name]['Test Accuracy']:.2f}\", 1)\n","        pdf.cell(25, 10, f\"{report['weighted avg']['precision']:.2f}\", 1)\n","        pdf.cell(25, 10, f\"{report['weighted avg']['recall']:.2f}\", 1)\n","        pdf.cell(25, 10, f\"{report['weighted avg']['f1-score']:.2f}\", 1)\n","        pdf.cell(25, 10, f\"{roc_auc:.2f}\", 1)\n","        pdf.cell(25, 10, f\"{avg_precision:.2f}\", 1)\n","        pdf.ln()\n","    pdf.ln(10)\n","\n","    # Add results for each model\n","    for model_name in results:\n","        pdf.add_page()\n","        pdf.set_font(\"Arial\", 'B', 16)\n","        pdf.cell(0, 10, f\"{model_name} - Results\", 0, 1, \"C\")\n","        pdf.ln(5)\n","\n","        # Accuracy Info\n","        acc = results[model_name][\"Test Accuracy\"]\n","        paper_acc = paper_results[model_name]\n","        pdf.set_font(\"Arial\", '', 12)\n","        pdf.cell(0, 10, f\"Test Accuracy: {acc:.2f}%\", 0, 1)\n","        pdf.cell(0, 10, f\"Paper Accuracy: {paper_acc:.2f}%\", 0, 1)\n","        pdf.cell(0, 10, f\"Difference: {acc - paper_acc:+.2f}%\", 0, 1)\n","        pdf.ln(5)\n","\n","        # Training history plot\n","        if os.path.exists(f\"{model_name}_training_history.png\"):\n","            pdf.set_font(\"Arial\", 'B', 12)\n","            pdf.cell(0, 10, \"Training History\", 0, 1)\n","            pdf.image(f\"{model_name}_training_history.png\", x=10, y=30, w=180)\n","            pdf.ln(85)\n","\n","        # Confusion Matrix\n","        if os.path.exists(f\"{model_name}_confusion_matrix.png\"):\n","            pdf.add_page()\n","            pdf.set_font(\"Arial\", 'B', 12)\n","            pdf.cell(0, 10, \"Confusion Matrix\", 0, 1)\n","            pdf.image(f\"{model_name}_confusion_matrix.png\", x=10, y=20, w=180)\n","            pdf.ln(85)\n","\n","        # ROC Curve\n","        if os.path.exists(f\"{model_name}_roc_curve.png\"):\n","            pdf.add_page()\n","            pdf.set_font(\"Arial\", 'B', 12)\n","            pdf.cell(0, 10, \"ROC Curve\", 0, 1)\n","            pdf.image(f\"{model_name}_roc_curve.png\", x=10, y=20, w=180)\n","            pdf.ln(85)\n","\n","        # Precision-Recall Curve\n","        if os.path.exists(f\"{model_name}_pr_curve.png\"):\n","            pdf.add_page()\n","            pdf.set_font(\"Arial\", 'B', 12)\n","            pdf.cell(0, 10, \"Precision-Recall Curve\", 0, 1)\n","            pdf.image(f\"{model_name}_pr_curve.png\", x=10, y=20, w=180)\n","            pdf.ln(85)\n","\n","        # Grad-CAM visualization if available\n","        if 'Grad-CAM' in model_name and os.path.exists(f\"{model_name}_grad_cam.png\"):\n","            pdf.add_page()\n","            pdf.set_font(\"Arial\", 'B', 12)\n","            pdf.cell(0, 10, \"Grad-CAM Visualization\", 0, 1)\n","            pdf.image(f\"{model_name}_grad_cam.png\", x=10, y=20, w=180)\n","            pdf.ln(85)\n","\n","        # Classification Report\n","        pdf.add_page()\n","        pdf.set_font(\"Arial\", 'B', 12)\n","        pdf.cell(0, 10, \"Classification Report\", 0, 1)\n","\n","        report = results[model_name][\"Classification Report\"]\n","\n","        # Table headers\n","        pdf.set_font(\"Arial\", 'B', 10)\n","        pdf.cell(40, 10, \"Class\", 1)\n","        pdf.cell(30, 10, \"Precision\", 1)\n","        pdf.cell(30, 10, \"Recall\", 1)\n","        pdf.cell(30, 10, \"F1-Score\", 1)\n","        pdf.cell(30, 10, \"Support\", 1)\n","        pdf.ln()\n","\n","        pdf.set_font(\"Arial\", '', 10)\n","        for cls in dataset.classes:\n","            pdf.cell(40, 10, cls, 1)\n","            pdf.cell(30, 10, f\"{report[cls]['precision']:.2f}\", 1)\n","            pdf.cell(30, 10, f\"{report[cls]['recall']:.2f}\", 1)\n","            pdf.cell(30, 10, f\"{report[cls]['f1-score']:.2f}\", 1)\n","            pdf.cell(30, 10, str(report[cls]['support']), 1)\n","            pdf.ln()\n","\n","        # Add macro avg\n","        pdf.cell(40, 10, \"Macro Avg\", 1)\n","        pdf.cell(30, 10, f\"{report['macro avg']['precision']:.2f}\", 1)\n","        pdf.cell(30, 10, f\"{report['macro avg']['recall']:.2f}\", 1)\n","        pdf.cell(30, 10, f\"{report['macro avg']['f1-score']:.2f}\", 1)\n","        pdf.cell(30, 10, str(report['macro avg']['support']), 1)\n","        pdf.ln()\n","\n","        # Weighted avg\n","        pdf.cell(40, 10, \"Weighted Avg\", 1)\n","        pdf.cell(30, 10, f\"{report['weighted avg']['precision']:.2f}\", 1)\n","        pdf.cell(30, 10, f\"{report['weighted avg']['recall']:.2f}\", 1)\n","        pdf.cell(30, 10, f\"{report['weighted avg']['f1-score']:.2f}\", 1)\n","        pdf.cell(30, 10, str(report['weighted avg']['support']), 1)\n","        pdf.ln()\n","\n","    # Save final PDF\n","    pdf.output(\"Comprehensive_Report.pdf\")\n","    print(\"✅ Comprehensive report saved as Comprehensive_Report.pdf\")\n","\n","# Visualize the dataset\n","print(\"Sample images from each class:\")\n","visualize_samples(dataset)\n","\n","print(\"Class distribution:\")\n","class_counts = plot_class_distribution(dataset)\n","for cls, count in zip(dataset.classes, class_counts):\n","    print(f\"{cls}: {count} images\")\n","\n","# Main execution\n","# Dictionary to store results\n","results = {}\n","\n","# Paper results for comparison\n","paper_results = {\n","    'AlexNet': 69.08,\n","    'ResNet': 91.33,\n","    'VGG-16': 84.92,\n","    'ViT': 98.50,\n","    'Swin Transformer': 96.55,\n","    'Fusion ResNet and VGG-16': 97.17,\n","    'Fusion ResNet and AlexNet': 97.65,\n","    'Fusion ResNet and AlexNet with Grad-CAM': 99.97\n","}\n","\n","# List of models to train\n","models_to_train = {\n","    'AlexNet': create_alexnet(),\n","    'ResNet': create_resnet(),\n","    'VGG-16': create_vgg16(),\n","    'ViT': create_vit(),\n","    'Swin Transformer': create_swin(),\n","    'Fusion ResNet and VGG-16': create_fusion_resnet_vgg16(),\n","    'Fusion ResNet and AlexNet': create_fusion_resnet_alexnet(),\n","    'Fusion ResNet and AlexNet with Grad-CAM': create_fusion_resnet_alexnet_gradcam()\n","}\n","\n","# Train and evaluate each model\n","for model_name, model in models_to_train.items():\n","    print(f\"\\n=== Training {model_name} ===\")\n","    best_val_acc, train_history = train_model(model, model_name, train_loader, val_loader, num_epochs=NUM_EPOCHS)\n","\n","    print(f\"\\n=== Evaluating {model_name} ===\")\n","    test_acc, report, probs, img_paths, preds, labels, roc_auc, avg_precision = evaluate_model(model, model_name, test_loader)\n","\n","    results[model_name] = {\n","        'Validation Accuracy': best_val_acc,\n","        'Test Accuracy': test_acc,\n","        'Train History': train_history,\n","        'Classification Report': report,\n","        'Probabilities': probs,\n","        'Image Paths': img_paths,\n","        'Predictions': preds,\n","        'Labels': labels,\n","        'ROC AUC': roc_auc,\n","        'Average Precision': avg_precision\n","    }\n","\n","    print(f\"{model_name} - Validation Accuracy: {best_val_acc:.2f}%, Test Accuracy: {test_acc:.2f}%\")\n","\n","    # Apply Grad-CAM for the fusion model with Grad-CAM\n","    if 'Grad-CAM' in model_name:\n","        apply_grad_cam(model, model_name, test_loader, results)\n","\n","# Create comprehensive PDF report at the end\n","create_comprehensive_pdf(results, paper_results, dataset, train_dataset, val_dataset, test_dataset)\n","\n","# Create a results directory and move all reports\n","!mkdir -p model_reports\n","!mv *.pdf model_reports/\n","!mv *.png model_reports/\n","!mv *.pth model_reports/\n","\n","print(\"\\nAll reports have been saved in the 'model_reports' directory\")\n","\n","# Print final comparison\n","print(\"\\n=== Final Comparison with Paper Results ===\")\n","print(\"Model\\t\\tOur Result\\tPaper Result\\tDifference\")\n","print(\"-\" * 55)\n","for model_name in results:\n","    our_acc = results[model_name]['Test Accuracy']\n","    paper_acc = paper_results[model_name]\n","    diff = our_acc - paper_acc\n","    print(f\"{model_name:25}\\t{our_acc:.2f}%\\t\\t{paper_acc:.2f}%\\t\\t{diff:+.2f}%\")"]}]}